# StockDigest Job Queue System

## Overview

The Job Queue system eliminates HTTP 520 errors by decoupling long-running ticker processing from HTTP request lifecycles. Processing happens server-side in a background worker, with PowerShell polling for status instead of maintaining long HTTP connections.

## Architecture

### Key Components

1. **PostgreSQL-Based Queue** - No Redis/Celery overhead, uses existing database
2. **Background Worker Thread** - Polls database for jobs, processes sequentially
3. **Circuit Breaker** - Detects system-wide failures and halts processing
4. **Timeout Watchdog** - Monitors and kills jobs that exceed time limits
5. **Ticker Isolation** - Maintains existing `TICKER_PROCESSING_LOCK` to prevent corruption

### Why This Fixes 520 Errors

**OLD (Broken):**
```
PowerShell â†’ HTTP â†’ /cron/digest (30 min processing) â†’ 520 timeout
```

**NEW (Fixed):**
```
PowerShell â†’ HTTP â†’ /jobs/submit (instant response)
PowerShell â†’ HTTP â†’ /jobs/batch/{id} (instant status check) Ã— N polls
Server Background Worker â†’ Process jobs â†’ Update DB
```

HTTP requests are now **always < 1 second**. No more timeouts.

## Database Schema

### `ticker_processing_batches`
Tracks groups of tickers submitted together.

**Fields:**
- `batch_id` (UUID, PK)
- `status` (queued | processing | completed | failed | cancelled)
- `total_jobs`, `completed_jobs`, `failed_jobs`
- Timestamps: `created_at`, `started_at`, `completed_at`
- `config` (JSONB) - Stores minutes, batch_size, triage_batch_size

### `ticker_processing_jobs`
Individual ticker jobs within a batch.

**Fields:**
- `job_id` (UUID, PK)
- `batch_id` (FK to batches)
- `ticker` (VARCHAR)
- `status` (queued | processing | completed | failed | cancelled | timeout)
- `phase` (ingest_start | ingest_complete | digest_start | etc.)
- `progress` (0-100)
- **Retry Logic:** `retry_count`, `max_retries`, `last_retry_at`
- **Results:** `result` (JSONB), `error_message`, `error_stacktrace`
- **Resource Tracking:** `worker_id`, `memory_mb`, `duration_seconds`
- **Timeout:** `timeout_at` (timestamp)
- `config` (JSONB) - Per-job configuration

### Indexes

```sql
CREATE INDEX idx_jobs_status_queued ON ticker_processing_jobs(status, created_at) WHERE status = 'queued';
CREATE INDEX idx_jobs_status_processing ON ticker_processing_jobs(status, timeout_at) WHERE status = 'processing';
```

Uses `FOR UPDATE SKIP LOCKED` for atomic job claiming (prevents race conditions).

## API Endpoints

### `POST /jobs/submit`

Submit a batch of tickers for processing.

**Request:**
```json
{
  "tickers": ["RY.TO", "TD.TO", "VST", "CEG"],
  "minutes": 4320,
  "batch_size": 3,
  "triage_batch_size": 3
}
```

**Response:**
```json
{
  "status": "submitted",
  "batch_id": "550e8400-e29b-41d4-a716-446655440000",
  "job_ids": ["...", "...", "...", "..."],
  "tickers": ["RY.TO", "TD.TO", "VST", "CEG"],
  "total_jobs": 4,
  "message": "Processing started server-side for 4 tickers"
}
```

### `GET /jobs/batch/{batch_id}`

Get status of all jobs in a batch.

**Response:**
```json
{
  "batch_id": "550e8400-e29b-41d4-a716-446655440000",
  "status": "processing",
  "total_tickers": 4,
  "completed": 2,
  "failed": 0,
  "processing": 1,
  "queued": 1,
  "overall_progress": 62,
  "jobs": [
    {
      "job_id": "...",
      "ticker": "RY.TO",
      "status": "completed",
      "phase": "complete",
      "progress": 100,
      "duration_seconds": 1834.5,
      "memory_mb": 234.2
    },
    ...
  ]
}
```

### `GET /jobs/{job_id}`

Get detailed status of a single job (includes stacktraces, full result).

### `POST /jobs/circuit-breaker/reset`

Manually reset the circuit breaker after resolving system issues.

### `GET /jobs/stats`

Get queue statistics (counts by status, avg duration, circuit breaker state).

## Background Worker

### Job Processing Flow

```python
1. Poll database: SELECT ... FOR UPDATE SKIP LOCKED (atomic claim)
2. Update status: processing
3. Acquire TICKER_PROCESSING_LOCK (ensures ticker isolation)
4. Phase 1: Ingest (RSS feeds, AI triage)
   â†’ Update: progress=60, phase=ingest_complete
5. Phase 2: Digest (scrape, AI analysis, send emails)
   â†’ Update: progress=95, phase=digest_complete
6. Phase 3: Commit to GitHub
   â†’ Update: progress=99, phase=commit_complete
7. Mark complete: status=completed, progress=100
8. Release lock
9. Poll for next job
```

### Circuit Breaker

**Purpose:** Detect system-wide failures (DB crashes, API outages) and stop processing to prevent cascading failures.

**Logic:**
- Failure threshold: 3 consecutive system failures
- State: `closed` (working) | `open` (failing)
- Reset timeout: 300 seconds (auto-close after 5 min)

**System failures:** Database connection errors, memory exhaustion, critical API failures
**Ticker failures:** Individual ticker processing errors (doesn't trigger circuit breaker)

### Timeout Watchdog

Separate thread that runs every 60 seconds:

```sql
UPDATE ticker_processing_jobs
SET status = 'timeout', error_message = 'Job exceeded timeout limit'
WHERE status = 'processing' AND timeout_at < NOW()
```

Default timeout: **45 minutes per ticker**

## PowerShell Client

### Usage

```powershell
.\scripts\setup_job_queue.ps1
```

### Flow

1. **Initialize tickers** (one-time, prevents corruption)
2. **Submit batch** â†’ Get `batch_id`
3. **Poll for status** every 20s
4. **Display results** when complete

### Output

```
=== STEP 2: SUBMITTING JOB BATCH ===
  âœ… Batch submitted: 550e8400-e29b-41d4-a716-446655440000
  Processing 4 tickers server-side...

=== STEP 3: MONITORING PROGRESS ===
  Progress: 75% | Completed: 3/4 | Failed: 0 | Processing: 1 | Current: CEG [digest_complete]

=== STEP 4: DETAILED RESULTS ===
  âœ… RY.TO: completed (30.5min) [mem: 234MB]
  âœ… TD.TO: completed (28.2min) [mem: 221MB]
  âœ… VST: completed (25.1min) [mem: 198MB]
  âœ… CEG: completed (27.8min) [mem: 215MB]

=== SUMMARY ===
  Total: 4 tickers
  Completed: 4
  Failed: 0
  Success Rate: 100.0%
```

## Production Features

### âœ… Retry Logic

Jobs can be retried up to `max_retries` (default: 2) on transient failures.

```sql
UPDATE ticker_processing_jobs
SET status = 'queued', retry_count = retry_count + 1
WHERE job_id = '...' AND retry_count < max_retries
```

### âœ… Resource Tracking

Every job records:
- `worker_id` - Which Render instance processed it
- `memory_mb` - Peak memory usage
- `duration_seconds` - Total processing time

### âœ… Audit Trail

Complete history:
- `created_at` - When job was submitted
- `started_at` - When worker claimed it
- `completed_at` - When it finished
- `error_stacktrace` - Full Python stacktrace on failure

### âœ… Queue Capacity Limits

Prevents queue overflow:

```python
if queued_count > 100:
    raise HTTPException(429, "Job queue is full")
```

## Migration from Old System

### Before (OLD System)

```powershell
# PowerShell orchestrates everything
foreach ($ticker in $TICKERS) {
    $ingest = Invoke-RestMethod .../cron/ingest?tickers=$ticker  # Blocks 15-30 min â†’ 520
    $digest = Invoke-RestMethod .../cron/digest?tickers=$ticker  # Blocks 15-30 min â†’ 520
}
```

**Problems:**
- HTTP 520 timeouts after 60-120s
- PowerShell crash = lost progress
- No visibility into failures

### After (NEW System)

```powershell
# PowerShell submits and monitors
$batch = Invoke-RestMethod .../jobs/submit  # < 1s
while ($notDone) {
    $status = Invoke-RestMethod .../jobs/batch/$batch_id  # < 1s
    Start-Sleep -Seconds 20
}
```

**Benefits:**
- âœ… No HTTP timeouts (requests are instant)
- âœ… PowerShell can crash/reconnect (state in DB)
- âœ… Real-time progress visibility
- âœ… Per-ticker error isolation
- âœ… Automatic retries
- âœ… Full audit trail

## Testing

### 1. Test with Single Ticker

```powershell
$TICKERS = @("RY.TO")
.\scripts\setup_job_queue.ps1
```

### 2. Monitor Logs

```bash
# Check Render logs for:
# - "ðŸ”§ Job worker started"
# - "ðŸ“‹ Claimed job ..."
# - "âœ… [JOB xxx] COMPLETED"
```

### 3. Check Database

```sql
-- See all jobs
SELECT job_id, ticker, status, phase, progress, duration_seconds
FROM ticker_processing_jobs
ORDER BY created_at DESC
LIMIT 10;

-- See circuit breaker state
SELECT * FROM ticker_processing_batches
WHERE status IN ('processing', 'completed')
ORDER BY created_at DESC
LIMIT 5;
```

### 4. Test Failure Scenarios

```sql
-- Simulate timeout
UPDATE ticker_processing_jobs
SET timeout_at = NOW() - INTERVAL '1 hour'
WHERE status = 'processing';

-- Watchdog will mark as timeout within 60 seconds
```

## Monitoring

### Key Metrics

```bash
# Queue depth
SELECT COUNT(*) FROM ticker_processing_jobs WHERE status = 'queued';

# Processing rate (jobs/hour)
SELECT COUNT(*) / 1.0 as jobs_per_hour
FROM ticker_processing_jobs
WHERE completed_at > NOW() - INTERVAL '1 hour';

# Failure rate
SELECT
    COUNT(CASE WHEN status = 'completed' THEN 1 END) as succeeded,
    COUNT(CASE WHEN status IN ('failed', 'timeout') THEN 1 END) as failed
FROM ticker_processing_jobs
WHERE created_at > NOW() - INTERVAL '24 hours';

# Average duration
SELECT AVG(duration_seconds) / 60.0 as avg_minutes
FROM ticker_processing_jobs
WHERE status = 'completed';
```

### Health Checks

```bash
# Check worker is running
GET /jobs/stats

# Response:
{
  "circuit_breaker_state": "closed",  # â† Should be "closed"
  "worker_id": "render-instance-123",
  "status_counts": {
    "completed": 145,
    "processing": 1,
    "queued": 0,
    "failed": 3
  }
}
```

## Troubleshooting

### Job stuck in "processing"

**Cause:** Worker crashed mid-job
**Solution:** Timeout watchdog will mark as timeout after 45 min

### Circuit breaker is OPEN

**Cause:** 3+ system failures detected
**Solution:**
1. Check Render logs for root cause
2. Fix issue (e.g., database connection)
3. Reset circuit breaker: `POST /jobs/circuit-breaker/reset`

### No jobs processing

**Check:**
1. Worker thread is running: `GET /jobs/stats` â†’ check `worker_id`
2. Queue has jobs: `SELECT * FROM ticker_processing_jobs WHERE status = 'queued'`
3. Circuit breaker is closed: `GET /jobs/stats` â†’ `circuit_breaker_state = 'closed'`

## Advanced Configuration

### Adjust Timeout

Edit `app.py`:

```python
timeout_minutes = 45  # Increase if needed
```

### Adjust Circuit Breaker

```python
job_circuit_breaker = CircuitBreaker(
    failure_threshold=3,   # Failures before opening
    reset_timeout=300      # Seconds before auto-close
)
```

### Add Email Alerts

In `CircuitBreaker.record_failure()`:

```python
if self.failure_count >= self.failure_threshold:
    send_alert_email("CRITICAL: Circuit breaker open!")
```

## FAQ

### Q: What happens if Render restarts the server?

**A:** Jobs in "processing" state will be marked as timeout by watchdog. Queued jobs remain queued and will be processed when worker restarts.

### Q: Can multiple workers run simultaneously?

**A:** Yes! `FOR UPDATE SKIP LOCKED` prevents race conditions. Each worker claims different jobs.

### Q: How do I prioritize certain tickers?

**A:** Add `priority INT` column to `ticker_processing_jobs`, update query:

```sql
ORDER BY priority DESC, created_at
```

### Q: Can I cancel a running job?

**A:** Add endpoint:

```python
@APP.post("/jobs/{job_id}/cancel")
def cancel_job(job_id: str):
    # Mark as cancelled
    # Worker checks status periodically and stops if cancelled
```

## Summary

**Before:** HTTP 520 errors, lost progress, no visibility
**After:** Reliable, resumable, observable processing

**Key Innovation:** PostgreSQL as job queue = 0 infrastructure cost, 100% reliability improvement

**Production Ready:** Retry logic, timeouts, circuit breaker, audit trail, resource tracking

**Next Steps:**
1. Test with 1 ticker
2. Test with 4 tickers
3. Monitor for 24 hours
4. Deploy to production